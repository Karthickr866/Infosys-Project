{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9102fc1-76f0-4a7f-bdbb-26c576466fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10557 files belonging to 4 classes.\n",
      "Using 8446 files for training.\n",
      "Found 10557 files belonging to 4 classes.\n",
      "Using 2111 files for validation.\n",
      "Epoch 1/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1658s\u001b[0m 6s/step - accuracy: 0.4475 - loss: 1.3665 - val_accuracy: 0.6665 - val_loss: 0.8396\n",
      "Epoch 2/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m947s\u001b[0m 4s/step - accuracy: 0.6354 - loss: 0.9040 - val_accuracy: 0.7039 - val_loss: 0.7600\n",
      "Epoch 3/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 2s/step - accuracy: 0.6775 - loss: 0.7953 - val_accuracy: 0.7721 - val_loss: 0.6361\n",
      "Epoch 4/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 2s/step - accuracy: 0.7250 - loss: 0.6988 - val_accuracy: 0.8157 - val_loss: 0.5280\n",
      "Epoch 5/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 2s/step - accuracy: 0.7588 - loss: 0.6197 - val_accuracy: 0.8290 - val_loss: 0.4973\n",
      "Epoch 6/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 2s/step - accuracy: 0.7856 - loss: 0.5600 - val_accuracy: 0.8631 - val_loss: 0.4215\n",
      "Epoch 7/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 2s/step - accuracy: 0.8135 - loss: 0.4843 - val_accuracy: 0.8603 - val_loss: 0.4072\n",
      "Epoch 8/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 2s/step - accuracy: 0.8276 - loss: 0.4660 - val_accuracy: 0.8835 - val_loss: 0.3490\n",
      "Epoch 9/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 2s/step - accuracy: 0.8449 - loss: 0.4201 - val_accuracy: 0.8868 - val_loss: 0.3360\n",
      "Epoch 10/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 3s/step - accuracy: 0.8668 - loss: 0.3572 - val_accuracy: 0.8977 - val_loss: 0.3170\n",
      "Epoch 11/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 3s/step - accuracy: 0.8740 - loss: 0.3443 - val_accuracy: 0.9038 - val_loss: 0.2762\n",
      "Epoch 12/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 3s/step - accuracy: 0.8807 - loss: 0.3197 - val_accuracy: 0.9176 - val_loss: 0.2624\n",
      "Epoch 13/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 3s/step - accuracy: 0.8920 - loss: 0.2939 - val_accuracy: 0.9185 - val_loss: 0.2443\n",
      "Epoch 14/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 3s/step - accuracy: 0.8936 - loss: 0.2877 - val_accuracy: 0.9199 - val_loss: 0.2357\n",
      "Epoch 15/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 3s/step - accuracy: 0.9071 - loss: 0.2518 - val_accuracy: 0.9190 - val_loss: 0.2508\n",
      "Epoch 16/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m729s\u001b[0m 3s/step - accuracy: 0.9124 - loss: 0.2368 - val_accuracy: 0.9204 - val_loss: 0.2287\n",
      "Epoch 17/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 2s/step - accuracy: 0.9108 - loss: 0.2511 - val_accuracy: 0.9256 - val_loss: 0.2315\n",
      "Epoch 18/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 2s/step - accuracy: 0.9239 - loss: 0.2131 - val_accuracy: 0.9360 - val_loss: 0.1970\n",
      "Epoch 19/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 2s/step - accuracy: 0.9237 - loss: 0.2135 - val_accuracy: 0.9247 - val_loss: 0.2088\n",
      "Epoch 20/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 2s/step - accuracy: 0.9284 - loss: 0.1958 - val_accuracy: 0.9384 - val_loss: 0.1920\n",
      "Epoch 21/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 2s/step - accuracy: 0.9299 - loss: 0.1895 - val_accuracy: 0.9365 - val_loss: 0.1853\n",
      "Epoch 22/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 2s/step - accuracy: 0.9343 - loss: 0.1834 - val_accuracy: 0.9351 - val_loss: 0.1970\n",
      "Epoch 23/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 2s/step - accuracy: 0.9385 - loss: 0.1825 - val_accuracy: 0.9375 - val_loss: 0.1878\n",
      "Epoch 24/30\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 2s/step - accuracy: 0.9400 - loss: 0.1683 - val_accuracy: 0.9337 - val_loss: 0.1910\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 1s/step - accuracy: 0.9418 - loss: 0.1825\n",
      "Validation Loss: 0.18528054654598236\n",
      "Validation Accuracy: 0.9365229606628418\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# Paths and settings\n",
    "dataset_path = r\"C:\\Users\\saite\\OneDrive\\Desktop\\AUGMENTED\"  # Adjust your dataset path\n",
    "batch_size = 32\n",
    "image_size = (160, 160)  # Smaller image resolution for faster training\n",
    "num_epochs = 30\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "# Set mixed precision policy\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Normalize pixel values\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Optimize data pipeline\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Load the base model with pre-trained weights\n",
    "base_model = MobileNetV2(input_shape=image_size + (3,), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Freeze the base model initially\n",
    "\n",
    "# Add custom layers\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.5),  # Regularization to prevent overfitting\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')  # Adjust to match the number of classes\n",
    "])\n",
    "\n",
    "# Compile the model with a learning rate schedule\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks: Early stopping and model checkpointing\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    \"best_model.keras\",  # Preferred format\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Fine-tuning: Unfreeze some layers of the base model\n",
    "base_model.trainable = True\n",
    "fine_tune_at = len(base_model.layers) // 2  # Unfreeze the top half layers\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Re-compile the model with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate / 10),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_epochs = 10  # Additional epochs for fine-tuning\n",
    "history_fine = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=fine_tune_epochs,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6066ca77-cbbb-4d7a-b055-377b7b59f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model\n",
    "model.save(\"final_model.keras\")  # Save in .keras format\n",
    "# Or if you prefer the .h5 format\n",
    "# model.save(\"final_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54eb122b-eed8-4498-85d4-d3eaca444951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Predicted Class: 3\n",
      "Confidence: 0.50390625\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess a single image\n",
    "def preprocess_image(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=image_size)\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    return img_array\n",
    "\n",
    "# Path to a new image\n",
    "image_path = r\"C:\\Users\\saite\\OneDrive\\Desktop\\8bd7b67e02fdfbe0ec875a8815d4bf26.jpg\"  \n",
    "\n",
    "# Preprocess the image\n",
    "processed_image = preprocess_image(image_path)\n",
    "\n",
    "# Make a prediction\n",
    "predictions = model.predict(processed_image)\n",
    "predicted_class = tf.argmax(predictions[0])  # Get the predicted class index\n",
    "confidence = tf.reduce_max(predictions[0])  # Get confidence of the prediction\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341de87-ea37-4966-89ce-7ccc8176100a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
